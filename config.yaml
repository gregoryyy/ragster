# Configuration without secrets

# parameters for local ollama
ollama:
  host: http://localhost:11434
  model: llama3.2:3b
  timeout: 60

# ingestion
ingest:
  chunk_size: 500
  retrieval_top_k: 5

# embedding
embed:
  # sentence transformer
  st_model: all-MiniLM-L6-v2

